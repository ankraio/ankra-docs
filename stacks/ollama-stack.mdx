---
title: "Deploy LLMs on Kubernetes"
description: "Learn how to deploy Large Language Models (LLMs) onto any Kubernetes cluster using LibreChat and Ollama."
---

In this tutorial, you'll learn how to deploy Large Language Models (LLMs) onto any Kubernetes cluster using LibreChat as the user interface and Ollama to run local models.

This setup lets you seamlessly switch between API-based models (like OpenAI or Google) and models running directly on your cluster—all within the same interface.

Let's dive in!

<div style={{ position: "relative", paddingBottom: "56.25%", height: 0, overflow: "hidden", maxWidth: "100%" }}>
  <iframe
    style={{ position: "absolute", top: 0, left: 0, width: "100%", height: "100%" }}
    src="https://www.youtube.com/embed/_H3wUM9yWjw"
    title="YouTube video player"
    frameBorder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowFullScreen
  />
</div>

---

### What you'll need

- A running Kubernetes cluster
- `kubectl` configured for your cluster
- A GitHub account and repository for configuration files
- API keys if you plan to use API-based models (e.g., OpenAI, Google)

---

### Step 1: Import or Create a Cluster in Ankra

If this is your first time using Ankra, see the [getting started guide](/quickstart).

**Verify Cluster Status:**
```bash
kubectl get nodes
```

---

### Step 2: Add LibreChat to Your Cluster

LibreChat will serve as the front-end for interacting with your LLMs.

**1. Apply API Key Secret**

LibreChat requires a Kubernetes secret containing your API keys. Ankra or the LibreChat add-on documentation will provide a `kubectl` command to create this secret.

```bash
kubectl create secret generic librechat-credentials \
  --from-literal=OPENAI_API_KEY='YOUR_OPENAI_KEY' \
  --from-literal=GOOGLE_API_KEY='YOUR_GOOGLE_KEY' \
  -n <namespace_for_libre_chat>
```
> Replace `YOUR_OPENAI_KEY`, `YOUR_GOOGLE_KEY`, and `<namespace_for_libre_chat>` with your actual values.

If you've already created this secret, you can apply it using:
```bash
kubectl apply -f your-secret-file.yaml
```

**2. Install LibreChat from Add-on Library**

- In Ankra, go to your cluster and open the "Add-on Library."
- Search for "LibreChat."
- Subscribe to the add-on and the community profile "Librechat + Mongo + Meilisearch."
- Go to "Add-ons" → "Add" → select "LibreChat" → choose the profile → click "Add."

**3. Monitor Installation**

An operation will begin to install LibreChat. Check that the LibreChat pods are running:
```bash
kubectl get pods -n <namespace_where_libre_chat_is_installed>
```

---

### Step 3: Deploy Ollama to Run LLMs

Ollama lets you pull and run LLMs locally on your cluster. For this example, we'll deploy "deepseek-coder."

- In Ankra, go to "Add-ons" in your cluster.
- Find and select the "Ollama" add-on. See available models at the [Ollama library](https://ollama.com/library).
- Specify the model you want (e.g., "Deepseek Coder") and click "Add."
- Monitor installation and verify the Ollama pod is running:
```bash
kubectl get pods -n <namespace_where_ollama_is_installed>
```

---

### Step 4: Access LibreChat and Interact with Your Models

**1. Find LibreChat Service**

Get the external IP and port:
```bash
kubectl get services -n <namespace_where_libre_chat_is_installed>
```
Look for the LibreChat service and note its external IP and port.

**2. Open LibreChat in Your Browser**

Navigate to `http://<EXTERNAL_IP>:<PORT>` (e.g., `http://your.cluster.ip:30080`).

**3. Create an Account and Sign In**

You'll see the LibreChat interface. Create an account and sign in.

**4. Explore Your Models**

You should see both API-based models (if you configured the secret) and local models via Ollama (e.g., "Deepseek Coder").

**5. Test Your Local Model**

Select your Ollama model and send a prompt to ensure it's working.

---

Congratulations!
You've successfully deployed LLMs onto your Kubernetes cluster using LibreChat and Ollama. You now have a flexible AI stack that can run both cloud-based and local models.