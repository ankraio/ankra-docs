---
title: Alerts
description: Monitor your clusters with customizable alerts and notifications
---

<Note>
Ankra Alerts help you stay informed about your cluster health, resource issues, and operational events with configurable notifications.
</Note>

## What are Alerts?

Alerts in Ankra let you:

- **Monitor Cluster Health:** Get notified when clusters go offline or experience issues
- **Track Resource Status:** Watch for pod failures, deployment problems, and resource quota issues
- **Receive Notifications:** Send alerts to Slack via webhooks
- **View Alert History:** Review past alerts and their resolutions

---

## Alert Components

### Alert Rules

Alert rules define **what** triggers an alert:

- **Condition:** The event or state that triggers the alert
- **Severity:** Critical, Warning, or Info
- **Scope:** Which clusters or resources to monitor
- **Thresholds:** Numeric values that trigger alerts (e.g., CPU > 80%)

### Alert Webhooks

Webhooks define **where** alerts are sent:

- **Slack:** Post alerts to a Slack channel via incoming webhook

---

## Creating Alert Rules

<Steps>
  <Step title="Navigate to Alerts">
    Go to your cluster and click **Alerts** in the sidebar, or access organization-wide alerts from your organization settings.
  </Step>
  <Step title="Create New Rule">
    Click **Create Alert Rule** and configure:
    - **Name:** A descriptive name for the rule
    - **Condition:** Select the event type or metric
    - **Severity:** Choose Critical, Warning, or Info
    - **Clusters:** Select which clusters to monitor
  </Step>
  <Step title="Add Conditions">
    Define specific conditions for the alert:
    - Resource state changes (e.g., pod becomes unhealthy)
    - Metric thresholds (e.g., memory usage > 90%)
    - Event occurrences (e.g., deployment failed)
  </Step>
  <Step title="Configure Notifications">
    Select which webhooks should receive this alert. You can send different severity levels to different channels.
  </Step>
  <Step title="Save and Enable">
    Save the rule. It becomes active immediately and will trigger based on your conditions.
  </Step>
</Steps>

---

## Setting Up Webhooks

<Steps>
  <Step title="Navigate to Webhooks">
    Go to **Alerts → Webhooks** in your cluster or organization settings.
  </Step>
  <Step title="Add Webhook">
    Click **Add Webhook** and enter your Slack incoming webhook URL.
    
    To create a Slack webhook, go to your Slack workspace's **Apps → Incoming Webhooks** and create a new webhook for your desired channel.
  </Step>
  <Step title="Test the Webhook">
    Click **Test** to send a sample alert and verify the configuration.
  </Step>
  <Step title="Assign to Rules">
    When creating alert rules, select this webhook as a notification target.
  </Step>
</Steps>

---

## Alert Types

<CardGroup cols={2}>
  <Card title="Cluster Connectivity" icon="signal">
    <ul>
      <li>Cluster goes offline</li>
      <li>Agent disconnected</li>
      <li>Connection timeout</li>
    </ul>
  </Card>
  <Card title="Workload Health" icon="heart-pulse">
    <ul>
      <li>Pod crash loops</li>
      <li>Deployment rollout failed</li>
      <li>Container OOMKilled</li>
    </ul>
  </Card>
  <Card title="Resource Usage" icon="gauge-high">
    <ul>
      <li>CPU threshold exceeded</li>
      <li>Memory pressure</li>
      <li>Storage quota reached</li>
    </ul>
  </Card>
  <Card title="Operations" icon="gears">
    <ul>
      <li>Stack deployment failed</li>
      <li>Add-on installation error</li>
      <li>GitOps sync failed</li>
    </ul>
  </Card>
</CardGroup>

---

## Alert Severity Levels

| Level | Description | Use Case |
|-------|-------------|----------|
| **Critical** | Immediate action required | Production outages, data loss risks |
| **Warning** | Attention needed soon | Resource pressure, degraded performance |
| **Info** | Informational updates | Successful deployments, routine events |

---

## Viewing Alert History

Access your alert history to:

- **Review Past Alerts:** See when alerts triggered and resolved
- **Analyze Patterns:** Identify recurring issues
- **Audit Responses:** Track how alerts were handled
- **Export Data:** Download alert history for reporting

Navigate to **Alerts → History** to view the timeline of all alerts.

---

## Best Practices

<Tip>
**Start Conservative:** Begin with fewer, broader alerts and refine as you learn your cluster's normal behavior.
</Tip>

<Tip>
**Use Severity Wisely:** Reserve Critical for true emergencies to avoid alert fatigue.
</Tip>

<Tip>
**Route by Severity:** Send Critical alerts to PagerDuty, Warnings to Slack, and Info to email.
</Tip>

<Tip>
**Review Regularly:** Periodically review and tune alert thresholds based on actual incidents.
</Tip>

---

## Prometheus Integration

If your cluster has Prometheus installed, Ankra can forward metrics queries:

1. Configure your Prometheus endpoint in cluster settings
2. Create alert rules based on PromQL queries
3. Ankra evaluates the queries and triggers alerts based on results

This allows you to leverage existing Prometheus metrics with Ankra's alerting and notification system.

---

Still have questions? [Join our Slack community](https://ankra.io/slack) and we'll help out.
