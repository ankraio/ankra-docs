---
title: AI Assistant
description: AI-powered incident triangulation combining logs, metrics, manifests, and deployment context
---

<Note>
Press `⌘+J` (Mac) or `Ctrl+J` (Windows/Linux) to open the AI Assistant from anywhere in Ankra.
</Note>

<Frame>
  <img src="/images/ai-chat.png" alt="AI Assistant interface" />
</Frame>

The AI Assistant combines everything Ankra knows about your cluster-logs, Kubernetes manifests, Stack deployments, resource states, and events-into a unified context for intelligent troubleshooting. This makes it exceptionally powerful for incident triangulation, connecting symptoms across multiple layers to find root causes fast.

<CardGroup cols={2}>
  <Card title="Page-Aware" icon="eye">
    Automatically focuses on what you're viewing-open a pod and the AI knows its logs, manifest, and status.
  </Card>
  <Card title="Unified Context" icon="layer-group">
    Correlates logs, manifests, Stack configurations, and resource states in one conversation.
  </Card>
  <Card title="Incident Triangulation" icon="crosshairs">
    Connects symptoms across pods, services, and deployments to pinpoint root causes.
  </Card>
  <Card title="Stack-Aware" icon="cubes">
    Understands your CD pipeline-which Stacks deployed what, when, and with which values.
  </Card>
</CardGroup>

---

## Page-Aware Context

The AI Assistant automatically knows what you're looking at. When you open the chat, it focuses on your current view:

| You're Viewing | AI Automatically Knows |
|----------------|------------------------|
| **A Pod** | Its logs, manifest, events, resource usage, and parent deployment |
| **A Deployment** | All replicas, rollout status, associated services, and recent changes |
| **A Stack** | Installed add-ons, Helm values, deployment history, and dependencies |
| **Logs View** | The filtered logs, error patterns, and related resources |
| **A Service** | Endpoints, selectors, connected pods, and ingress rules |

This means you don't need to explain context-just ask your question:

- *Looking at a crashing pod:* "Why is this failing?" → AI already sees the logs and events
- *Viewing a deployment:* "Scale this to 5 replicas" → AI knows which deployment
- *On the Stack page:* "Add Redis to this stack" → AI knows the current stack configuration

<Tip>
The AI focuses on what you focus on. Navigate to a resource before asking about it for the most relevant answers.
</Tip>

---

## The Superpower: Combined Context

What makes Ankra's AI different is the unified environment. When you ask a question, the AI has access to:

| Context Layer | What the AI Sees |
|---------------|------------------|
| **Pod Logs** | Real-time and historical container logs with error patterns |
| **Kubernetes Manifests** | Your actual deployed YAML-not just documentation |
| **Stack Deployments** | Which Helm charts were deployed, their values, and versions |
| **Resource States** | Current status, events, conditions, and health |
| **Relationships** | Service → Deployment → Pod → Container dependencies |
| **Timeline** | When deployments happened and what changed |

This combined view enables questions like:

- *"The API is returning 500 errors. Was anything deployed recently that could cause this?"*
- *"Compare the current nginx config to what was running yesterday"*
- *"Which Stack change caused the database connection failures?"*

---

## Building Stacks with AI Assistance

The AI Assistant helps you plan and optimize Stacks:

<Steps>
  <Step title="Describe What You Need">
    Tell the AI what you want to deploy: "I need a monitoring stack with Prometheus, Grafana, and alerting"
  </Step>
  <Step title="Get Recommendations">
    The AI recommends add-ons, configurations, and dependencies based on best practices.
  </Step>
  <Step title="Build Your Stack">
    Add the recommended components to the Stack Builder and configure values based on the guidance.
  </Step>
  <Step title="Troubleshoot Deployments">
    When something fails: "The Grafana pod won't start after I deployed the monitoring stack"
  </Step>
</Steps>

### Example Stack Building Conversations

<AccordionGroup>
  <Accordion title="Creating a Production Stack">
    **You:** "I need to set up a production-ready ingress with TLS"
    
    **AI:** "I recommend adding these components to your Stack:
    1. **cert-manager** - For automatic TLS certificate management
    2. **ingress-nginx** - Production-grade ingress controller
    
    Here's the dependency order and suggested values for your cluster size..."
  </Accordion>
  <Accordion title="Optimizing Existing Stacks">
    **You:** "My monitoring stack is using too much memory"
    
    **AI:** "Looking at your current Prometheus configuration in the 'observability' Stack, I see retention is set to 30 days with no resource limits. Based on your cluster's 200 pods, I recommend:
    - Set `prometheus.retention` to 15d
    - Add `resources.limits.memory: 4Gi`
    - Enable `remote_write` to offload historical data..."
  </Accordion>
</AccordionGroup>

---

## Incident Triangulation

When something goes wrong, the AI correlates signals across your entire stack:

<Steps>
  <Step title="Identify Symptoms">
    "Users are reporting slow API responses"
  </Step>
  <Step title="Cross-Reference Logs">
    The AI checks pod logs for errors, timeouts, and latency patterns.
  </Step>
  <Step title="Check Recent Deployments">
    "I see the 'backend' Stack was updated 2 hours ago with a new database connection pool setting..."
  </Step>
  <Step title="Analyze Resource States">
    "The postgres pod is showing high CPU and connection queue buildup..."
  </Step>
  <Step title="Provide Root Cause">
    "The connection pool was reduced from 100 to 10 in the last Stack deployment, causing connection exhaustion under load."
  </Step>
</Steps>

### What the AI Triangulates

| Signal | How It's Used |
|--------|--------------|
| **Error Logs** | Pattern matching across all pods in the affected service chain |
| **Stack History** | Recent deployments and value changes that correlate with incident timing |
| **Resource Events** | Kubernetes events showing restarts, OOMs, and scheduling failures |
| **Dependencies** | Service mesh, database connections, and external integrations |
| **Configuration Drift** | Differences between current manifests and last known good state |

---

## What Can You Ask?

<AccordionGroup>
  <Accordion title="Incident Response">
    - "Why is the checkout service timing out?"
    - "What changed in the last hour that could cause this?"
    - "Compare pod logs before and after the deployment"
    - "Which upstream service is causing the 503 errors?"
  </Accordion>
  <Accordion title="Stack Building">
    - "Help me create a logging stack with Loki and Promtail"
    - "What's the best way to configure ingress for multiple domains?"
    - "How should I set up database backups in my Stack?"
    - "Add monitoring to my existing application Stack"
  </Accordion>
  <Accordion title="Configuration Analysis">
    - "Is my resource limit configuration correct for this workload?"
    - "Why is this HPA not scaling?"
    - "Explain the network policies affecting this service"
    - "What secrets does this deployment need?"
  </Accordion>
  <Accordion title="Root Cause Analysis">
    - "Why did this pod get OOMKilled?"
    - "What's causing intermittent connection resets?"
    - "The deployment rollout is stuck-what's blocking it?"
    - "Why are requests failing only to certain pod replicas?"
  </Accordion>
</AccordionGroup>

---

## Getting Started

<Steps>
  <Step title="Open the AI Assistant">
    Click the **chat icon** in the bottom-right corner of any cluster page, or use the Command Palette (`⌘+K` / `Ctrl+K`) and search for "AI Chat".
  </Step>
  <Step title="Ask Your Question">
    Type your question in natural language. The assistant understands context about your current cluster and can help with:
    - "Why is my pod crashing?"
    - "Explain this deployment configuration"
    - "How do I set up ingress?"
    - "What's wrong with this service?"
  </Step>
  <Step title="Review the Response">
    The AI provides detailed explanations, code examples, and actionable steps. You can ask follow-up questions to dive deeper.
  </Step>
  <Step title="Provide Feedback">
    Use the thumbs up/down buttons to rate responses. Your feedback helps improve the assistant over time.
  </Step>
</Steps>

---

## Key Features

<CardGroup cols={2}>
  <Card title="Context-Aware" icon="brain">
    The assistant understands your current cluster, namespace, and the resources you're viewing for more relevant answers.
  </Card>
  <Card title="Troubleshooting Mode" icon="wrench">
    Click "Troubleshoot" on any failing resource to get an AI-powered analysis of what's wrong and how to fix it.
  </Card>
  <Card title="Chat History" icon="clock-rotate-left">
    Your conversations are saved and searchable. Access previous chats from the Command Palette or chat panel.
  </Card>
  <Card title="Multiple AI Models" icon="microchip">
    Choose from different AI models based on your needs-faster responses or more detailed analysis.
  </Card>
</CardGroup>

---

## Troubleshooting Resources

When viewing a Kubernetes resource (pod, deployment, service, etc.), you can click the **Troubleshoot** button to start an AI-assisted diagnosis:

1. Navigate to the resource in the Kubernetes browser
2. Click **Troubleshoot** in the resource details
3. The AI will analyze:
   - Current resource state and events
   - Related resources and dependencies
   - Recent changes and logs
4. Receive actionable recommendations

---

## Chat History

Access your previous conversations:

- **Command Palette:** Press `⌘+K` and search for "Chat History"
- **Chat Panel:** Click the history icon in the chat header
- **Search:** Find past conversations by keyword

Conversations are organized by date and can be resumed at any time.

---

## Best Practices

<Tip>
**Be Specific:** Include error messages, resource names, and namespaces for more accurate help.
</Tip>

<Tip>
**Use Context:** When you're viewing a specific resource and open the chat, the AI already knows what you're looking at.
</Tip>

<Tip>
**Ask Follow-ups:** The AI remembers the conversation context, so you can ask clarifying questions.
</Tip>

---

## Privacy & Data

- Conversations are stored securely and associated with your account
- Cluster metadata may be shared for context (resource names, states, events)
- Sensitive data like secrets or credentials are never sent to the AI
- You can delete your chat history at any time

---

Still have questions? [Join our Slack community](https://ankra.io/slack) and we'll help out.
